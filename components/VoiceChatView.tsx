import React, { useState, useEffect, useRef, useCallback } from 'react';
import { GoogleGenAI, LiveServerMessage, Modality, Session, Blob } from "@google/genai";
import { createBlob, decode, decodeAudioData } from '../utils/audioUtils';

const AstronautAvatar: React.FC<{ isSpeaking: boolean, isListening: boolean }> = ({ isSpeaking, isListening }) => {
    return (
        <div className="relative w-48 h-48 sm:w-64 sm:h-64 mx-auto">
            <div className={`absolute inset-0 rounded-full bg-yellow-400 transition-all duration-500 ${isSpeaking ? 'animate-ping-slow opacity-75' : 'opacity-0'}`}></div>
            <div className={`absolute inset-0 rounded-full border-4 border-dashed border-teal-400 transition-all duration-500 ${isListening ? 'animate-spin-slow opacity-100' : 'opacity-0'}`}></div>
            <div className="relative z-10 w-full h-full bg-gray-700 rounded-full flex items-center justify-center text-8xl sm:text-9xl border-4 border-gray-600">
                ğŸ§‘â€ğŸš€
            </div>
        </div>
    );
};

const VoiceChatView: React.FC<{ onBack: () => void }> = ({ onBack }) => {
    const [status, setStatus] = useState('Ø¯Ø± Ø­Ø§Ù„ Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø±Ø§ÛŒ ØªÙ…Ø§Ø³...');
    const [error, setError] = useState<string | null>(null);
    const [isSpeaking, setIsSpeaking] = useState(false);
    const [isListening, setIsListening] = useState(false);
    const [userTranscript, setUserTranscript] = useState('');
    const [modelTranscript, setModelTranscript] = useState('');

    const sessionPromiseRef = useRef<Promise<Session> | null>(null);
    
    const cleanupRef = useRef<(() => void) | null>(null);

    const startConversation = useCallback(async () => {
        setError(null);
        setStatus('Ø¯Ø± Ø­Ø§Ù„ Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø¯Ø³ØªØ±Ø³ÛŒ Ø¨Ù‡ Ù…ÛŒÚ©Ø±ÙˆÙÙˆÙ†...');

        const ai = new GoogleGenAI({ apiKey: process.env.API_KEY });
        // Fix: Cast window to `any` to support `webkitAudioContext` for older browsers without TypeScript errors.
        const inputAudioContext = new (window.AudioContext || (window as any).webkitAudioContext)({ sampleRate: 16000 });
        // Fix: Cast window to `any` to support `webkitAudioContext` for older browsers without TypeScript errors.
        const outputAudioContext = new (window.AudioContext || (window as any).webkitAudioContext)({ sampleRate: 24000 });
        const outputNode = outputAudioContext.createGain();
        outputNode.connect(outputAudioContext.destination);
        const sources = new Set<AudioBufferSourceNode>();
        let nextStartTime = 0;
        let stream: MediaStream | null = null;
        let scriptProcessor: ScriptProcessorNode | null = null;
        let mediaSource: MediaStreamAudioSourceNode | null = null;
        
        const cleanup = () => {
          if (cleanupRef.current === cleanup) {
              inputAudioContext.close();
              outputAudioContext.close();
              if (stream) {
                  stream.getTracks().forEach(track => track.stop());
              }
              if(scriptProcessor) {
                  scriptProcessor.disconnect();
              }
              if(mediaSource) {
                  mediaSource.disconnect();
              }
              sessionPromiseRef.current?.then(session => session.close());
              setIsListening(false);
              setIsSpeaking(false);
              console.log("Cleanup complete.");
          }
        };
        cleanupRef.current = cleanup;

        try {
            stream = await navigator.mediaDevices.getUserMedia({ 
                audio: {
                    sampleRate: 16000,
                    channelCount: 1,
                }
            });
            
            setStatus('Ø¯Ø± Ø­Ø§Ù„ Ø¨Ø±Ù‚Ø±Ø§Ø±ÛŒ Ø§Ø±ØªØ¨Ø§Ø· Ø¨Ø§ ÙØ¶Ø§...');

            const systemInstruction = "ØªÙˆ ÛŒÚ© ÙØ¶Ø§Ù†ÙˆØ±Ø¯ Ø´Ø¬Ø§Ø¹ Ùˆ Ù…Ø§Ø¬Ø±Ø§Ø¬Ùˆ Ù‡Ø³ØªÛŒ Ú©Ù‡ Ø¨Ù‡ Ù‡Ù…Ù‡ Ø¬Ø§ Ø³ÙØ± Ú©Ø±Ø¯Ù‡! ØªÙˆ Ø¯Ø§Ø³ØªØ§Ù†â€ŒÙ‡Ø§ÛŒ Ù‡ÛŒØ¬Ø§Ù†â€ŒØ§Ù†Ú¯ÛŒØ² Ø³ÙØ± Ø¨Ù‡ Ø³ØªØ§Ø±Ù‡â€ŒÙ‡Ø§ Ùˆ Ø³ÛŒØ§Ø±Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù Ø±Ùˆ Ø¨Ø±Ø§ÛŒ Ø¨Ú†Ù‡â€ŒÙ‡Ø§ ØªØ¹Ø±ÛŒÙ Ù…ÛŒâ€ŒÚ©Ù†ÛŒ. Ù‡Ø¯Ù ØªÙˆ Ø§ÛŒÙ†Ù‡ Ú©Ù‡ Ø¨Ú†Ù‡â€ŒÙ‡Ø§ Ø±Ùˆ Ø¨Ù‡ ÙØ¶Ø§ Ùˆ Ø¹Ù„Ù… Ø¹Ù„Ø§Ù‚Ù‡â€ŒÙ…Ù†Ø¯ Ú©Ù†ÛŒ. Ø¨Ø§ Ø¨Ú†Ù‡â€ŒÙ‡Ø§ Ø­Ø±Ù Ù…ÛŒâ€ŒØ²Ù†ÛŒ Ù¾Ø³ Ø®ÛŒÙ„ÛŒ Ø³Ø§Ø¯Ù‡ØŒ Ø´Ø§Ø¯ Ùˆ Ø¯ÙˆØ³ØªØ§Ù†Ù‡ ØµØ­Ø¨Øª Ú©Ù†. Ø¬ÙˆØ§Ø¨â€ŒÙ‡Ø§Øª Ú©ÙˆØªØ§Ù‡ Ùˆ Ù‡ÛŒØ¬Ø§Ù†â€ŒØ§Ù†Ú¯ÛŒØ² Ø¨Ø§Ø´Ù‡! Ù…ÛŒâ€ŒØªÙˆÙ†ÛŒ Ø§Ø² Ø´Ú©Ù„Ú©â€ŒÙ‡Ø§ÛŒ Ù…ØªÙ†ÛŒ Ù…Ø«Ù„ :) ÛŒØ§ :D Ù‡Ù… Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒ. Ø§Ø² Ú©Ù„Ù…Ù‡â€ŒÙ‡Ø§ÛŒ Ø³Ø®Øª Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù†Ú©Ù†.";

            sessionPromiseRef.current = ai.live.connect({
                model: 'gemini-2.5-flash-native-audio-preview-09-2025',
                callbacks: {
                    onopen: () => {
                        setStatus('Ø§Ø±ØªØ¨Ø§Ø· Ø¨Ø±Ù‚Ø±Ø§Ø± Ø´Ø¯! ØµØ­Ø¨Øª Ú©Ù†...');
                        setIsListening(true);
                        mediaSource = inputAudioContext.createMediaStreamSource(stream!);
                        scriptProcessor = inputAudioContext.createScriptProcessor(4096, 1, 1);
                        
                        scriptProcessor.onaudioprocess = (audioProcessingEvent) => {
                            const inputData = audioProcessingEvent.inputBuffer.getChannelData(0);
                            const pcmBlob = createBlob(inputData);
                            sessionPromiseRef.current?.then((session) => {
                                session.sendRealtimeInput({ media: pcmBlob });
                            });
                        };
                        mediaSource.connect(scriptProcessor);
                        scriptProcessor.connect(inputAudioContext.destination);
                    },
                    onmessage: async (message: LiveServerMessage) => {
                        if (message.serverContent?.outputTranscription) {
                            setIsSpeaking(true);
                            setModelTranscript(prev => prev + message.serverContent.outputTranscription.text);
                        } else if (message.serverContent?.inputTranscription) {
                            setUserTranscript(prev => prev + message.serverContent.inputTranscription.text);
                        }

                        if (message.serverContent?.turnComplete) {
                            setIsSpeaking(false);
                            setUserTranscript('');
                            setModelTranscript('');
                        }
                        
                        const base64EncodedAudioString = message.serverContent?.modelTurn?.parts[0]?.inlineData?.data;
                        if (base64EncodedAudioString) {
                            nextStartTime = Math.max(nextStartTime, outputAudioContext.currentTime);
                            const audioBuffer = await decodeAudioData(decode(base64EncodedAudioString), outputAudioContext, 24000, 1);
                            const source = outputAudioContext.createBufferSource();
                            source.buffer = audioBuffer;
                            source.connect(outputNode);
                            source.addEventListener('ended', () => {
                                sources.delete(source);
                                if (sources.size === 0) setIsSpeaking(false);
                            });
                            source.start(nextStartTime);
                            nextStartTime = nextStartTime + audioBuffer.duration;
                            sources.add(source);
                        }

                        if (message.serverContent?.interrupted) {
                            for (const source of sources.values()) {
                                source.stop();
                                sources.delete(source);
                            }
                            nextStartTime = 0;
                            setIsSpeaking(false);
                        }
                    },
                    onerror: (e: ErrorEvent) => {
                        setError(`Ø§ÙˆÙ‡! Ø§Ø±ØªØ¨Ø§Ø· Ø¨Ø§ Ø§ÛŒØ³ØªÚ¯Ø§Ù‡ ÙØ¶Ø§ÛŒÛŒ Ù‚Ø·Ø¹ Ø´Ø¯: ${e.message}`);
                        setIsListening(false);
                        setIsSpeaking(false);
                        cleanup();
                    },
                    onclose: (e: CloseEvent) => {
                        setStatus('ØªÙ…Ø§Ø³ Ø¨Ù‡ Ù¾Ø§ÛŒØ§Ù† Ø±Ø³ÛŒØ¯.');
                        setIsListening(false);
                        setIsSpeaking(false);
                        cleanup();
                    },
                },
                config: {
                    responseModalities: [Modality.AUDIO],
                    speechConfig: { voiceConfig: { prebuiltVoiceConfig: { voiceName: 'Zephyr' } } },
                    systemInstruction: systemInstruction,
                    inputAudioTranscription: {},
                    outputAudioTranscription: {},
                },
            });

        } catch (err) {
            console.error(err);
            setError('Ù†ØªÙˆÙ†Ø³ØªÙ… Ø¨Ù‡ Ù…ÛŒÚ©Ø±ÙˆÙÙˆÙ† Ø¯Ø³ØªØ±Ø³ÛŒ Ù¾ÛŒØ¯Ø§ Ú©Ù†Ù…. Ø§Ø¬Ø§Ø²Ù‡ Ø¯Ø§Ø¯ÛŒØŸ');
            setStatus('Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø³ØªØ±Ø³ÛŒ Ø¨Ù‡ Ù…ÛŒÚ©Ø±ÙˆÙÙˆÙ†');
        }
    }, []);

    useEffect(() => {
        startConversation();
        return () => {
            cleanupRef.current?.();
        };
    }, [startConversation]);

    return (
        <div className="flex flex-col h-screen bg-gray-900 text-white p-4 sm:p-6 items-center justify-center animate-fade-in">
            <div className="w-full max-w-4xl text-center">
                <h1 className="text-4xl sm:text-5xl font-display text-transparent bg-clip-text bg-gradient-to-r from-yellow-400 to-orange-500 mb-6">
                    ØªÙ…Ø§Ø³ ØµÙˆØªÛŒ Ø¨Ø§ ÙØ¶Ø§Ù†ÙˆØ±Ø¯
                </h1>

                <AstronautAvatar isSpeaking={isSpeaking} isListening={isListening} />

                <p className="text-xl text-yellow-300 mt-6 h-8">{error || status}</p>

                <div className="mt-6 p-4 bg-gray-800/50 rounded-2xl min-h-[100px] text-right text-lg">
                    <p className="text-gray-400">
                        <span className="font-bold text-white">ØªÙˆ: </span>
                        {userTranscript || "..."}
                    </p>
                    <p className="text-teal-300 mt-2">
                         <span className="font-bold text-white">ÙØ¶Ø§Ù†ÙˆØ±Ø¯: </span>
                         {modelTranscript || "..."}
                    </p>
                </div>

                <div className="mt-8 flex flex-col sm:flex-row justify-center items-center gap-4">
                     <button
                        onClick={onBack}
                        className="w-full sm:w-auto bg-gradient-to-r from-red-500 to-orange-600 hover:from-red-600 hover:to-orange-700 text-white font-bold py-4 px-10 rounded-full text-2xl transition-all duration-300 transform hover:scale-105 font-display"
                    >
                        Ù¾Ø§ÛŒØ§Ù† ØªÙ…Ø§Ø³
                    </button>
                    <button onClick={startConversation} className="w-full sm:w-auto bg-gray-600 hover:bg-gray-500 text-white font-bold py-3 px-6 rounded-full text-lg transition-colors">
                        Ø±Ø§Ù‡ Ø§Ù†Ø¯Ø§Ø²ÛŒ Ù…Ø¬Ø¯Ø¯
                    </button>
                </div>
            </div>
             <style>{`
                @keyframes fade-in {
                    from { opacity: 0; }
                    to { opacity: 1; }
                }
                .animate-fade-in {
                    animation: fade-in 0.5s ease-in-out forwards;
                }
                @keyframes ping-slow {
                    0%, 100% { transform: scale(1); opacity: 0.5; }
                    50% { transform: scale(1.1); opacity: 0.75; }
                }
                .animate-ping-slow {
                    animation: ping-slow 2s cubic-bezier(0, 0, 0.2, 1) infinite;
                }
                .animate-spin-slow {
                    animation: spin 3s linear infinite;
                }
            `}</style>
        </div>
    );
};

export default VoiceChatView;